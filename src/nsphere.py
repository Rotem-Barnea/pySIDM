from typing import Any, Unpack, TypedDict, NotRequired
from pathlib import Path

import numpy as np
import regex
import pandas as pd
from astropy import table
from numpy.typing import NDArray
from astropy.units import Quantity

from tqdm import tqdm


class File_params(TypedDict):
    """Parameter dictionary for loading `NSphere-SIDM` data files."""

    base_filename: str
    ntimesteps: int
    tfinal: int
    max_time: NotRequired[Quantity['time']]
    root_path: NotRequired[str | Path]


# Define the record dtype (as in your original code)
record_dtype = {
    'Rank_Mass_Rad_VRad_unsorted': np.dtype(
        [
            ('rank', np.int32),
            ('mass', np.float32),
            ('R', np.float32),
            ('Vrad', np.float32),
            ('PsiA', np.float32),
            ('E', np.float32),
            ('L', np.float32),
        ]
    )
}


def gather_files(
    base_filename: str,
    ntimesteps: int,
    tfinal: int,
    max_time: Quantity['time'] = Quantity(1, 'Gyr'),
    root_path: str | Path = r'../../NSphere-SIDM/data/',
) -> pd.DataFrame:
    """Gather all files for a given run.

    The filename structure is given by `root_path/{base_filename}_t*_100000_{ntimesteps+1}_{tfinal}.dat`.

    Parameters:
        base_filename: The base filename of the files to gather.
        ntimesteps: The number of timesteps in the run.
        tfinal: The total requested runtime of the run (in dynamical timescales).
        max_time: The maximum runtime of the simulation (in actual years).
        root_path: The root path of the files.

    Returns
        A dataframe containing the paths of the files and record dtypes.

    """
    if not isinstance(root_path, Path):
        root_path = Path(root_path)
    files = pd.DataFrame({'path': list(root_path.glob(f'{base_filename}_t*_100000_{ntimesteps + 1}_{tfinal}.dat'))})
    files['save_step'] = files.path.apply(get_save_step)
    files['time'] = files['save_step'].apply(lambda x: x / files['save_step'].max() * max_time)
    files['record_dtype'] = record_dtype.get(str(base_filename), {})
    return files.sort_values('time')


def get_save_step(path: Path) -> int:
    """Retrieve the save step from the file name."""
    return int(regex.findall(r'_t(\d+)_', path.stem)[0])


def load_file(path: str | Path, dtype: np.dtype[Any]) -> NDArray[Any]:
    """Load the file using np.fromfile."""
    return np.fromfile(path, dtype=dtype)


def load_all_files(files: pd.DataFrame | None = None, **kwargs: Unpack[File_params]) -> pd.DataFrame:
    """Load all data from all files.

    Parameters:
        files: A dataframe pointing to the files to be loaded, must include the columns 'path' and 'record_dtype'. If `None` the files dataframe will be generated by calling `gather_files()`.
        kwargs: Keyword arguments passed to `gather_files()` in the event files is `None`.

    Returns:
        A dataframe containing all the loaded data (concatenated).
    """
    if files is None:
        files = gather_files(**kwargs)
    data: list[pd.DataFrame] = []
    for path, dtype, time, save_step in tqdm(
        files[['path', 'record_dtype', 'time', 'save_step']].to_numpy(), desc='Load files'
    ):
        sub = pd.DataFrame(load_file(path, dtype))
        sub['time'] = time
        sub['save_step'] = save_step
        data += [sub]
    return pd.concat(data, ignore_index=True)


def to_snapshot_like(data: pd.DataFrame) -> table.QTable:
    """Convert the data to a format compatible with the halo snapshots.

    This is intended to allow the data to be plugged into the halo snapshots for convenient plotting.
    """
    data = data.rename(columns={'Vrad': 'vr', 'R': 'r', 'save_step': 'step'})
    data['vp'] = data['L'] / data['r']
    data['vx'] = data['vp']
    data['vy'] = 0
    data['v_norm'] = np.sqrt(data['vp'] ** 2 + data['vr'] ** 2)
    data['particle_type'] = 'dm'
    data = data.drop(columns=['rank', 'mass', 'PsiA', 'L'])
    return table.QTable.from_pandas(
        data,
        units={
            'r': 'kpc',
            'vx': 'kpc/Myr',
            'vy': 'kpc/Myr',
            'vr': 'kpc/Myr',
            'vp': 'kpc/Myr',
            'v_norm': 'kpc/Myr',
            'time': 'Myr',
            'E': 'kpc^2/Myr^2',
        },
    )


def prepare_for_plotting(data: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame, Quantity]:
    """Prepare the data for plotting."""
    data['vp'] = data['L'] / data['R']
    data['v_norm'] = np.sqrt(data['vp'] ** 2 + data['Vrad'] ** 2)
    data['T'] = 1 / 2 * data['v_norm'] ** 2
    data['particle_index'] = 0
    for _, group in data.groupby('time'):
        data.loc[group.index, 'particle_index'] = np.arange(len(group))
    data = data.rename(columns={'R': 'r', 'Vrad': 'vr'})
    initial_data = data[data['time'] == data['time'].min()].copy().sort_values('r')
    unit_mass = Quantity(initial_data['mass'].diff(1).iloc[-1], 'Msun')
    return data, initial_data, unit_mass


def read_log(
    filepath: str | Path,
    pattern: str = r'time=([\d.]+) Myr.+, n_scatters=(\d+)',
    columns: list[str] = ['time', 'cumulative_scatters'],
    append_zero: bool = False,
) -> pd.DataFrame:
    """Read the NSphere log file and return a DataFrame."""
    results = []
    with open(filepath) as f:
        for line in f.readlines():
            finds = regex.findall(pattern, line)
            if finds:
                results += [finds[0]]
    output = pd.DataFrame(np.array(results, dtype=float), columns=columns)
    if append_zero:
        output = pd.concat([pd.DataFrame([[0] * len(columns)], columns=columns), output], ignore_index=True)
    return output
